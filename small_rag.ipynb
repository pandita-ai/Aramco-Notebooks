{"cells":[{"cell_type":"markdown","id":"gkd0cU8MxFlq","metadata":{"id":"gkd0cU8MxFlq"},"source":["Authored by: Aryan Mistry"]},{"cell_type":"markdown","id":"40834063","metadata":{"id":"40834063"},"source":["# Minimal Retrieval-Augmented Generation (RAG) Pipeline\n","\n","Large language models are excellent at pattern matching but they can **hallucinate** by making up facts that aren't grounded in reality. One way to reduce hallucinations is to **augment the model with retrieval**: instead of relying solely on its internal parameters, the model retrieves relevant passages from a knowledge base and uses them to inform its answer.\n","\n","This notebook walks you through a minimal Retrieval‑Augmented Generation (RAG) pipeline. We'll start with a tiny corpus, break it into chunks, embed those chunks so we can compare them to a question, retrieve the most relevant pieces, and finally generate an answer (without using a real model). Along the way we'll highlight why each step matters and how libraries like LangChain can help you build production systems. [9]"]},{"cell_type":"markdown","id":"26164733","metadata":{"id":"26164733"},"source":["## 1. Sample Corpus\n","\n","We'll work with a small corpus of short articles about various topics. In a real\n","application you would index thousands of documents, but this toy corpus is\n","sufficient to illustrate the retrieval process.\n","\n","Run the cell below to define the corpus and inspect a few entries. Each string\n","represents a document.\n"]},{"cell_type":"code","execution_count":1,"id":"8dd4c07b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1760120875866,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"8dd4c07b","outputId":"1bc6a0de-5962-4350-bcc9-e044459d9a52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Document 0: The Apollo 11 mission in 1969 marked the first time humans walked on the Moon. Neil Armstrong and Bu...\n","Document 1: Global climate change is primarily driven by human activities such as burning fossil fuels. Rising c...\n","Document 2: Artificial intelligence (AI) refers to computer systems that can perform tasks normally requiring hu...\n","Document 3: The global economy is interconnected. Events in one country, like interest rate changes or trade pol...\n","Document 4: Photosynthesis is the process by which green plants convert sunlight into chemical energy. Chlorophy...\n"]}],"source":["\n","# Define a small corpus of documents\n","corpus = [\n","    \"The Apollo 11 mission in 1969 marked the first time humans walked on the Moon. \"\n","    \"Neil Armstrong and Buzz Aldrin spent hours exploring the lunar surface while Michael Collins orbited above.\",\n","\n","    \"Global climate change is primarily driven by human activities such as burning fossil fuels. \"\n","    \"Rising concentrations of greenhouse gases are warming the atmosphere and oceans, leading to melting ice caps.\",\n","\n","    \"Artificial intelligence (AI) refers to computer systems that can perform tasks normally requiring human intelligence. \"\n","    \"Machine learning is a subset of AI in which algorithms improve through experience.\",\n","\n","    \"The global economy is interconnected. Events in one country, like interest rate changes or trade policies, \"\n","    \"can ripple through financial markets around the world.\",\n","\n","    \"Photosynthesis is the process by which green plants convert sunlight into chemical energy. \"\n","    \"Chlorophyll in plant cells captures light energy to fuel this conversion.\"\n","]\n","\n","# Preview the corpus\n","for i, doc in enumerate(corpus):\n","    print(f\"Document {i}: {doc[:100]}...\")\n"]},{"cell_type":"markdown","id":"9e9e6e83","metadata":{"id":"9e9e6e83"},"source":["## 2. Chunking\n","\n","Language models have a limited context window: they can't ingest arbitrarily\n","long documents at once. To make retrieval efficient and to ensure relevant\n","snippets are available, we split each document into *chunks*. Here we'll\n","implement a simple chunker that splits on full stops and groups sentences\n","into pairs. In a production system you might use more sophisticated methods\n","(e.g. sentence tokenisers, fixed token counts or overlap between chunks).\n","\n","Feel free to experiment with the `max_sentences` parameter in the exercises\n","below. [9]\n"]},{"cell_type":"code","execution_count":2,"id":"beaba1fd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1760120875867,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"beaba1fd","outputId":"fb9c3457-5ee2-481c-c540-eb417492880b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Created 5 chunks from 5 documents.\n","Chunk 0 (doc 0): The Apollo 11 mission in 1969 marked the first time humans walked on the Moon. N...\n","Chunk 1 (doc 1): Global climate change is primarily driven by human activities such as burning fo...\n","Chunk 2 (doc 2): Artificial intelligence (AI) refers to computer systems that can perform tasks n...\n","Chunk 3 (doc 3): The global economy is interconnected. Events in one country, like interest rate ...\n","Chunk 4 (doc 4): Photosynthesis is the process by which green plants convert sunlight into chemic...\n"]}],"source":["\n","import re\n","\n","# Split a document into chunks containing up to `max_sentences` sentences.\n","# This simple splitter looks for periods and groups sentences into pairs.\n","def chunk_document(doc: str, max_sentences: int = 2) -> list:\n","    \"\"\"Split a document into chunks of up to `max_sentences` sentences.\n","\n","    This simplistic splitter uses periods to identify sentence boundaries. In practice you might use NLP libraries (e.g. spaCy) for more robust splitting.\n","    \"\"\"\n","    sentences = [s.strip() for s in re.split(r\"\\.(?!\\d)\", doc) if s.strip()]\n","    chunks = []\n","    for i in range(0, len(sentences), max_sentences):\n","        chunk = '. '.join(sentences[i:i + max_sentences]).strip()\n","        chunks.append(chunk)\n","    return chunks\n","\n","# Build the list of chunks along with their document IDs\n","chunks = []\n","for doc_id, doc in enumerate(corpus):\n","    for chunk in chunk_document(doc, max_sentences=2):\n","        chunks.append((doc_id, chunk))\n","\n","print(f\"Created {len(chunks)} chunks from {len(corpus)} documents.\")\n","for i, (doc_id, chunk) in enumerate(chunks[:5]):\n","    print(f\"Chunk {i} (doc {doc_id}): {chunk[:80]}...\")\n"]},{"cell_type":"markdown","id":"e711661f","metadata":{"id":"e711661f"},"source":["## 3. Embedding with TF-IDF\n","\n","To compare text snippets numerically we need vector representations. A simple\n","and widely used technique is **TF-IDF** (term frequency–inverse document\n","frequency). Each chunk is represented by a vector of term counts scaled by\n","how unique the term is across the corpus.\n","\n","While modern RAG systems use dense neural embeddings (e.g. from BERT or\n","OpenAI models) to capture semantic similarity, TF-IDF is easy to compute\n","and works surprisingly well on small corpora.\n","\n","We'll build a vocabulary, compute document frequencies, and then create a\n","TF-IDF vector for each chunk. [7]\n"]},{"cell_type":"code","execution_count":3,"id":"6557729f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1760120875871,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"6557729f","outputId":"bd83cf76-8ec5-467e-8f83-1d45439bdbd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computed TF-IDF vectors for 5 chunks and 114 terms.\n"]}],"source":["\n","import math\n","from collections import Counter, defaultdict\n","\n","# Build a vocabulary and document frequency dictionary\n","def build_vocabulary(chunks):\n","    \"\"\"Construct the vocabulary and document frequency counts for a list of chunks.\n","\n","    Returns a set of all unique terms and a dictionary mapping each term to the number of chunks it appears in.\n","    \"\"\"\n","    df = defaultdict(int)\n","    vocab = set()\n","    for _, chunk in chunks:\n","        words = chunk.lower().split()\n","        unique_words = set(words)\n","        for word in unique_words:\n","            df[word] += 1\n","        vocab.update(words)\n","    return sorted(vocab), df\n","\n","vocab, df = build_vocabulary(chunks)\n","\n","# Compute a TF-IDF vector for a single chunk\n","def compute_tfidf(chunk: str, vocab: list, df: dict, n_docs: int) -> list:\n","    \"\"\"Compute a TF‑IDF vector for a given chunk.\n","\n","    The TF term weights are raw counts and the IDF uses `df` to down‑weight common words.\n","    \"\"\"\n","    words = chunk.lower().split()\n","    tf = Counter(words)\n","    vec = []\n","    for term in vocab:\n","        tf_val = tf.get(term, 0)\n","        idf = math.log((n_docs + 1) / (df.get(term, 0) + 1)) + 1\n","        vec.append(tf_val * idf)\n","    return vec\n","\n","# Precompute all chunk vectors\n","n_chunks = len(chunks)\n","chunk_vectors = [compute_tfidf(text, vocab, df, n_chunks) for _, text in chunks]\n","print(f\"Computed TF-IDF vectors for {n_chunks} chunks and {len(vocab)} terms.\")\n"]},{"cell_type":"markdown","id":"c405011d","metadata":{"id":"c405011d"},"source":["## 4. Retrieval with Cosine Similarity\n","\n","With vector representations in hand, the next step is to find which chunks\n","are most relevant to a user's query. We'll convert the query into a TF-IDF\n","vector (using the same vocabulary and IDF values) and compute the **cosine\n","similarity** between the query and each chunk. Cosine similarity measures\n","angular similarity between vectors and ranges from –1 (opposite) to 1\n","(identical). We then pick the top-`k` chunks with the highest similarity.\n","\n","Feel free to adjust `top_k` to see how the retrieved context changes. [9]\n"]},{"cell_type":"code","execution_count":4,"id":"135b5b1d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1760120875874,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"135b5b1d","outputId":"0bccdca5-d952-4ddb-d894-84157da231e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: How do plants convert sunlight into energy?\n","Rank 1 (score=0.440): Document 4, chunk: Photosynthesis is the process by which green plants convert sunlight into chemical energy. Chlorophyll in plant cells captures light energy to fuel this conversion\n","Rank 2 (score=0.000): Document 0, chunk: The Apollo 11 mission in 1969 marked the first time humans walked on the Moon. Neil Armstrong and Buzz Aldrin spent hours exploring the lunar surface while Michael Collins orbited above\n","Rank 3 (score=0.000): Document 1, chunk: Global climate change is primarily driven by human activities such as burning fossil fuels. Rising concentrations of greenhouse gases are warming the atmosphere and oceans, leading to melting ice caps\n"]}],"source":["\n","# Compute cosine similarity between two vectors\n","def cosine_similarity(vec1: list, vec2: list) -> float:\n","    \"\"\"Compute cosine similarity between two numeric vectors.\n","\n","    Returns a value between 0 and 1 indicating how similar the two vectors are.\n","    \"\"\"\n","    dot = sum(a * b for a, b in zip(vec1, vec2))\n","    norm1 = math.sqrt(sum(a * a for a in vec1))\n","    norm2 = math.sqrt(sum(b * b for b in vec2))\n","    return dot / (norm1 * norm2 + 1e-8)\n","\n","# Search for top-k similar chunks\n","def search(query: str, chunks, chunk_vectors, vocab, df, n_docs, top_k: int = 2):\n","    q_vec = compute_tfidf(query, vocab, df, n_docs)\n","    sims = [cosine_similarity(q_vec, c_vec) for c_vec in chunk_vectors]\n","    ranked = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)\n","    return ranked[:top_k], [sims[i] for i in ranked[:top_k]]\n","\n","# Test retrieval on a sample query\n","query = \"How do plants convert sunlight into energy?\"\n","top_indices, similarities = search(query, chunks, chunk_vectors, vocab, df, n_chunks, top_k=3)\n","print(f\"Query: {query}\")\n","for rank, (idx, score) in enumerate(zip(top_indices, similarities), 1):\n","    doc_id, text = chunks[idx]\n","    print(f\"Rank {rank} (score={score:.3f}): Document {doc_id}, chunk: {text}\")\n"]},{"cell_type":"markdown","id":"fd7be164","metadata":{"id":"fd7be164"},"source":["## 5. Generation\n","\n","Once you've retrieved relevant context, a language model would normally read\n","both the query and the context and produce a natural language answer. In our\n","minimal example we'll simulate this by simply echoing the most relevant chunk\n","back to the user. This may feel unsatisfying, but it highlights the role of\n","retrieval: it surfaces the raw information on which a model could base its\n","answer. [9]\n"]},{"cell_type":"code","execution_count":5,"id":"31a896c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1760120876253,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"31a896c5","outputId":"72ed04b8-6cad-4685-957c-66bfe02b8947"},"outputs":[{"output_type":"stream","name":"stdout","text":["Based on document 4, here's some relevant information: Photosynthesis is the process by which green plants convert sunlight into chemical energy. Chlorophyll in plant cells captures light energy to fuel this conversion\n"]}],"source":["\n","# Generate an answer from the top retrieved context\n","def generate_answer(query: str, top_indices: list, chunks: list) -> str:\n","    \"\"\"Generate a naive answer by concatenating retrieved chunks.\n","\n","    In a real RAG system this function would call a language model, passing the query and retrieved context to generate a concise answer.\n","    \"\"\"\n","    if not top_indices:\n","        return \"I'm sorry, I couldn't find any relevant information to answer your question.\"\n","    doc_id, context = chunks[top_indices[0]]\n","    return f\"Based on document {doc_id}, here's some relevant information: {context}\"\n","\n","answer = generate_answer(query, top_indices, chunks)\n","print(answer)\n"]},{"cell_type":"markdown","id":"5ef12522","metadata":{"id":"5ef12522"},"source":["## 6. Towards Production: Using Libraries like LangChain\n","\n","Our implementation above exposes each part of the RAG pipeline. Frameworks\n","like **LangChain**, **LlamaIndex** and others provide higher-level abstractions\n","that handle chunking, embedding, retrieval and calling the language model\n","for you. Here's what using a RAG pipeline might look like with such a\n","framework (example code only—this cell is not executable here):\n","\n","```python\n","from langchain.document_loaders import TextLoader\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA\n","\n","# Load and chunk documents\n","loader = TextLoader('my_docs/*.txt')\n","raw_docs = loader.load()\n","\n","# Create an embedding model and vector store\n","embeddings = OpenAIEmbeddings()\n","vector_store = FAISS.from_documents(raw_docs, embeddings)\n","\n","# Build a retrieval-augmented QA chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=OpenAI(),\n","    chain_type=\"stuff\",\n","    retriever=vector_store.as_retriever()\n",")\n","\n","# Ask questions\n","response = qa_chain.run('How do plants convert sunlight into energy?')\n","print(response)\n","```\n","\n","These libraries take care of details like pagination, token limits and\n","embedding storage. Understanding the internals, however, helps you debug and\n","customise these systems. [11]\n"]},{"cell_type":"markdown","id":"6ec38b2c","metadata":{"id":"6ec38b2c"},"source":["## 7. Exercises\n","\n","To deepen your understanding, try the following exercises:\n","\n","1. **Vary the chunk size.** Change `max_sentences` in the chunking function to 1 or 3 and observe how retrieval results differ. Does a smaller or larger chunk size improve relevance?\n","2. **Top-`k` tuning.** Adjust `top_k` in the `search` function. What happens when you retrieve more than one chunk? How could you combine information from multiple chunks?\n","3. **Alternate similarity measures.** Modify `cosine_similarity` to use Euclidean distance or Jaccard similarity. Which works better on this corpus?\n","4. **Extend generation.** Instead of simply returning the top chunk, write a function that concatenates the top-`k` chunks and summarises them in your own words.\n","5. **Use a different corpus.** Replace the sample corpus with your own documents (e.g. lecture notes or articles) and build a mini RAG system for a new domain.\n","\n","Feel free to add cells below this one for your solutions. Use Markdown to describe your thought process and code cells to implement your answers.\n","\n","\n","6. **Implement Jaccard similarity retrieval.** Instead of TF‑IDF and cosine similarity, compute the Jaccard similarity between the set of words in the query and each chunk. Compare the retrieval results.\n","\n","HINT: Computing Jaccard Similarity\n","\n","To measure how similar two sets of words (e.g. query tokens and document tokens) are,  \n","you can use the **Jaccard similarity** formula:\n","\n","$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n","\n","Where:\n","\n","* \\(A\\) = set of unique tokens in text A  \n","* \\(B\\) = set of unique tokens in text B  \n","* \\(A ∩ B\\) = tokens present in **both** A and B  \n","* \\(A ∪ B\\) = tokens present in **either** A or B  \n","\n","In Python terms:\n","\n","```python\n","intersection = len(set_a & set_b)\n","union = len(set_a | set_b)\n","similarity = intersection / union\n","```\n","\n","7. **Use scikit‑learn's TF‑IDF.** Explore using `sklearn.feature_extraction.text.TfidfVectorizer` to compute embeddings. How do the results differ from our manual implementation?\n","8. **Expand the corpus.** Add more documents to the corpus and observe how the retrieval quality changes."]},{"cell_type":"markdown","source":["Foundational LLMs & Transformers\n","1. Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems (NIPS 2017).\n","2. Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS 2020.\n","3. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT 2019.\n","4. OpenAI (2023). GPT-4 Technical Report. arXiv:2303.08774.\n","5. Touvron, H., et al. (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. Meta AI.\n","\n","Generative AI & Sampling\n","\n","6. Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS 2014.\n","7. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n","8. Neal, R. M. (1993). Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical Report CRG-TR-93-1, University of Toronto.\n","\n","Retrieval-Augmented Generation (RAG) & Knowledge Grounding\n","\n","9. Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP. NeurIPS 2020.\n","10. deepset ai (2023). Haystack: Open-Source Framework for Search and RAG Applications. https://haystack.deepset.ai\n","11. LangChain (2023). LangChain Documentation and Cookbook. https://python.langchain.com\n","\n","Evaluation & Safety\n","\n","12. Papineni, K., et al. (2002). BLEU: A Method for Automatic Evaluation of Machine Translation. ACL 2002.\n","13. Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. ACL Workshop 2004.\n","14. OpenAI (2024). Evaluating Model Outputs: Faithfulness and Grounding. OpenAI Docs.\n","15. Guardrails AI (2024). Open-Source Guardrails Framework. https://github.com/shreyar/guardrails\n","\n","Prompt Engineering & Instruction Tuning\n","\n","16. White, J. (2023). The Prompting Guide. https://www.promptingguide.ai\n","17. Ouyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. NeurIPS 2022.\n","\n","Agents & Tool Use\n","\n","18. Yao, S., et al. (2022). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629.\n","19. LangChain (2024). LangChain Agents and Tools Documentation.\n","20. Microsoft (2023). Semantic Kernel Developer Guide. https://learn.microsoft.com/en-us/semantic-kernel/\n","21. Google DeepMind (2024). Gemini Technical Report. arXiv:2312.11805.\n","\n","State, Memory & Orchestration\n","\n","22. LangGraph (2024). Stateful Agent Orchestration Framework. https://langchain-langgraph.vercel.app\n","23. Park, J. S., et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442.\n","\n","Pedagogical and Course Design References\n","\n","24. fast.ai (2023). fast.ai Deep Learning Course Notebooks. https://course.fast.ai\n","25. Ng, A. (2023). DeepLearning.AI Short Courses on Generative AI.\n","26. MIT 6.S191, Stanford CS324, UC Berkeley CS294-158. (2022–2024). Course Materials and Public Notebooks for ML and LLMs."],"metadata":{"id":"UeO6YP2BgFH4"},"id":"UeO6YP2BgFH4"},{"cell_type":"code","source":[],"metadata":{"id":"G1ECCo-1gFl_","executionInfo":{"status":"ok","timestamp":1760120876255,"user_tz":420,"elapsed":1,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"}}},"id":"G1ECCo-1gFl_","execution_count":5,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}