{"cells":[{"cell_type":"markdown","id":"iciwPKnnfsXK","metadata":{"id":"iciwPKnnfsXK"},"source":["Authored by: Aryan Mistry"]},{"cell_type":"markdown","id":"33c33b4f","metadata":{"id":"33c33b4f"},"source":["# Tokenization, Inference and Text Completion\n","\n","Tokenization breaks a string into meaningful units that a language model can process. Inference uses a trained model to compute outputs such as next‑token probabilities, and *completion* refers to generating additional text given a prompt. In this expanded lab you will practise different levels of tokenization, build n‑gram models for completion, and explore how to use pre‑trained models (in a reference code block). [3]"]},{"cell_type":"markdown","id":"34b2b03b","metadata":{"id":"34b2b03b"},"source":["## 1 – Tokenization\n","\n","Different tokenization schemes affect how models interpret text:\n","\n","- **Whitespace or word tokenization** splits on spaces and punctuation. It is simple but struggles with contractions and unknown words.\n","- **Subword tokenization** (e.g. Byte‑Pair Encoding, WordPiece, SentencePiece) segments rare words into smaller units, allowing a manageable vocabulary while still capturing word structure.\n","- **Character tokenization** treats each character as a token and is robust to unseen words, but sequences become much longer.\n","\n","Below we implement simple word and character tokenizers using regular expressions and Python string methods. [3]"]},{"cell_type":"code","execution_count":null,"id":"cf1bedd6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1759552406056,"user":{"displayName":"Aryan Mistry","userId":"05905357547208756410"},"user_tz":420},"id":"cf1bedd6","outputId":"376554d2-7d67-4956-ed6e-1cf9c7c0e484"},"outputs":[{"name":"stdout","output_type":"stream","text":["Word tokens: ['tokenization', 'is', 'the', 'first', 'step', 'in', 'building', 'language', 'models']\n","Character tokens: ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ']\n"]}],"source":["import re\n","\n","def word_tokenize(text: str):\n","    return re.findall(r'\\b\\w+\\b', text.lower())\n","\n","def char_tokenize(text: str):\n","    return list(text.lower())\n","\n","example = 'Tokenization is the first step in building language models!'\n","print('Word tokens:', word_tokenize(example))\n","print('Character tokens:', char_tokenize(example)[:20])  # show first 20 characters"]},{"cell_type":"markdown","id":"2fd3028d","metadata":{"id":"2fd3028d"},"source":["### Exercise: Token Frequencies\n","\n","Using the `word_tokenize` function, compute the frequency of each word in the following short paragraph. Display the 5 most common words and their counts."]},{"cell_type":"code","execution_count":null,"id":"1d64e7ac","metadata":{"id":"1d64e7ac"},"outputs":[],"source":["paragraph = (\n","    'Large language models have revolutionized natural language processing. ' +\n","    'These models are trained on vast amounts of text data and can generate coherent sentences. ' +\n","    'However, they still rely on basic tokenization techniques to break text into manageable pieces.'\n",")\n","\n","# TODO: count word frequencies and display the top 5\n","# from collections import Counter\n","# tokens = word_tokenize(paragraph)\n","# word_counts = Counter(tokens)\n","# top5 = word_counts.most_common(5)\n","# print(top5)\n"]},{"cell_type":"markdown","id":"d3a31e6f","metadata":{"id":"d3a31e6f"},"source":["## 2 – Building a Markov Completion Model\n","\n","We revisit the Markov (bigram) model as a simple way to perform next‑word inference and completion. The functions below train a bigram model and generate completions. [7]"]},{"cell_type":"code","execution_count":null,"id":"ef910749","metadata":{"id":"ef910749"},"outputs":[],"source":["from collections import defaultdict, Counter\n","import random\n","\n","def train_bigram(tokens):\n","    counts = defaultdict(Counter)\n","    for i in range(len(tokens)-1):\n","        counts[tokens[i]][tokens[i+1]] += 1\n","    probs = {}\n","    for w1, counter in counts.items():\n","        total = sum(counter.values())\n","        probs[w1] = {w2: c/total for w2, c in counter.items()}\n","    return probs\n","\n","def generate_bigram(probs, start, length=10):\n","    current = start.lower()\n","    words = [current]\n","    for _ in range(length-1):\n","        next_dict = probs.get(current)\n","        if not next_dict:\n","            break\n","        candidates, p = zip(*next_dict.items())\n","        current = random.choices(candidates, p)[0]\n","        words.append(current)\n","    return ' '.join(words)\n","\n","# Train on the paragraph tokens\n","# tokens = word_tokenize(paragraph)\n","# bigram_probs = train_bigram(tokens)\n","# print('Completion starting from \"models\":', generate_bigram(bigram_probs, 'models'))"]},{"cell_type":"markdown","id":"dc0a0ed6","metadata":{"id":"dc0a0ed6"},"source":["### Exercise: Trigram Model and Perplexity\n","\n","1. **Trigram Model:** Modify the training function to build a trigram model (conditioning on two previous words). Use it to generate text and compare the coherence to the bigram model.\n","2. **Perplexity:** Implement a function that computes the perplexity of a given test sentence under your bigram model. Recall that perplexity for a sentence \\(w_1, \\dots, w_T\\) is defined as \\(\\exp(-\tfrac{1}{T}\\sum_{t} \\log P(w_t \\mid w_{t-1}))\\).\n","3. **Character vs Word Models:** Build a character‑level trigram model using `char_tokenize`. Compare its generated sequences to those of the word‑level trigram model."]},{"cell_type":"code","execution_count":null,"id":"911ef473","metadata":{"id":"911ef473"},"outputs":[],"source":["# TODO: implement trigram training and perplexity calculations here\n"]},{"cell_type":"markdown","id":"0b34820c","metadata":{"id":"0b34820c"},"source":["## 3 – Using Pre‑trained Models (Reference)\n","\n","Modern language models use subword tokenization and consider long contexts. To use a pre‑trained model such as GPT‑2 or GPT‑Neo, install the `transformers` library and run the code below. Because this environment does not include the package or weights, the following cell is provided as a reference only. [4]"]},{"cell_type":"code","execution_count":null,"id":"84fe5bca","metadata":{"id":"84fe5bca"},"outputs":[],"source":["# Reference code for using a Hugging Face model for completion\n","# !pip install transformers torch\n","# from transformers import AutoTokenizer, AutoModelForCausalLM\n","# import torch\n","\n","# model_name = 'gpt2'\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# prompt = 'The history of artificial intelligence begins'\n","# inputs = tokenizer(prompt, return_tensors='pt')\n","# outputs = model.generate(**inputs, max_length=30, do_sample=True, top_k=50, top_p=0.95)\n","# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","source":[],"metadata":{"id":"RwusrXF6B1Hx"},"id":"RwusrXF6B1Hx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Foundational LLMs & Transformers\n","1. Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems (NIPS 2017).\n","2. Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS 2020.\n","3. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT 2019.\n","4. OpenAI (2023). GPT-4 Technical Report. arXiv:2303.08774.\n","5. Touvron, H., et al. (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. Meta AI.\n","\n","Generative AI & Sampling\n","\n","6. Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS 2014.\n","7. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n","8. Neal, R. M. (1993). Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical Report CRG-TR-93-1, University of Toronto.\n","\n","Retrieval-Augmented Generation (RAG) & Knowledge Grounding\n","\n","9. Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP. NeurIPS 2020.\n","10. deepset ai (2023). Haystack: Open-Source Framework for Search and RAG Applications. https://haystack.deepset.ai\n","11. LangChain (2023). LangChain Documentation and Cookbook. https://python.langchain.com\n","\n","Evaluation & Safety\n","\n","12. Papineni, K., et al. (2002). BLEU: A Method for Automatic Evaluation of Machine Translation. ACL 2002.\n","13. Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. ACL Workshop 2004.\n","14. OpenAI (2024). Evaluating Model Outputs: Faithfulness and Grounding. OpenAI Docs.\n","15. Guardrails AI (2024). Open-Source Guardrails Framework. https://github.com/shreyar/guardrails\n","\n","Prompt Engineering & Instruction Tuning\n","\n","16. White, J. (2023). The Prompting Guide. https://www.promptingguide.ai\n","17. Ouyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. NeurIPS 2022.\n","\n","Agents & Tool Use\n","\n","18. Yao, S., et al. (2022). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629.\n","19. LangChain (2024). LangChain Agents and Tools Documentation.\n","20. Microsoft (2023). Semantic Kernel Developer Guide. https://learn.microsoft.com/en-us/semantic-kernel/\n","21. Google DeepMind (2024). Gemini Technical Report. arXiv:2312.11805.\n","\n","State, Memory & Orchestration\n","\n","22. LangGraph (2024). Stateful Agent Orchestration Framework. https://langchain-langgraph.vercel.app\n","23. Park, J. S., et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442.\n","\n","Pedagogical and Course Design References\n","\n","24. fast.ai (2023). fast.ai Deep Learning Course Notebooks. https://course.fast.ai\n","25. Ng, A. (2023). DeepLearning.AI Short Courses on Generative AI.\n","26. MIT 6.S191, Stanford CS324, UC Berkeley CS294-158. (2022–2024). Course Materials and Public Notebooks for ML and LLMs."],"metadata":{"id":"89e7OqiEgBev"},"id":"89e7OqiEgBev"},{"cell_type":"code","source":[],"metadata":{"id":"ng57duP4gCFv"},"id":"ng57duP4gCFv","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}