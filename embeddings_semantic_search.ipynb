{"cells":[{"cell_type":"markdown","id":"2FCP7kvBkkm3","metadata":{"id":"2FCP7kvBkkm3"},"source":["Authored by: Aryan Mistry"]},{"cell_type":"markdown","id":"95b3f09f","metadata":{"id":"95b3f09f"},"source":["# Embeddings and Semantic Search\n","\n","Representing text as numerical vectors (*embeddings*) allows us to compute similarities between documents and queries. In this lab you will build a simple retrieval system based on TF‑IDF vectors and cosine similarity, then read reference code for using dense embeddings via the `sentence-transformers` library. [9]"]},{"cell_type":"markdown","id":"3c4812af","metadata":{"id":"3c4812af"},"source":["## Part 1 – TF‑IDF Vectors\n","\n","TF‑IDF (Term Frequency–Inverse Document Frequency) is a classic technique for turning a collection of documents into numerical vectors. The *term frequency* (TF) of a word reflects how often it appears in a document, while the *inverse document frequency* (IDF) down‑weights words that appear in many documents. The TF‑IDF weight for a word *t* in document *d* is defined as:\n","\n","$$\\text{TF–IDF}(t, d) = \\text{TF}(t, d) \\times \\log\n","\\frac{N}{1 + n(t)}$$\n","\n","where **N** is the total number of documents and **n(t)** is the number of documents containing the word. Common words such as \"the\" or \"and\" have low IDF and therefore contribute less to the vector. Once each document is represented as a vector, we can compute similarities using the cosine of the angle between vectors.\n","\n","In the following code we build a simple TF‑IDF vectoriser, define a search function and then explore the highest‑weighted terms in each document. [7]"]},{"cell_type":"code","execution_count":3,"id":"942b0c11","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1760116102658,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"942b0c11","outputId":"9416ddf5-9ea7-40f0-e98b-bc1a786cb640"},"outputs":[{"output_type":"stream","name":"stdout","text":["Document 0 top terms: [('language', np.float64(0.6666666666666666)), ('natural', np.float64(0.3333333333333333)), ('models', np.float64(0.3333333333333333)), ('processing', np.float64(0.3333333333333333)), ('large', np.float64(0.3333333333333333))]\n","Document 1 top terms: [('words', np.float64(0.3779644730092272)), ('transformers', np.float64(0.3779644730092272)), ('use', np.float64(0.3779644730092272)), ('mechanisms', np.float64(0.3779644730092272)), ('relationships', np.float64(0.3779644730092272))]\n","Document 2 top terms: [('vectors', np.float64(0.388614292631317)), ('tf', np.float64(0.388614292631317)), ('baseline', np.float64(0.388614292631317)), ('provide', np.float64(0.388614292631317)), ('retrieval', np.float64(0.388614292631317))]\n","Document 3 top terms: [('word', np.float64(0.4217647821447532)), ('semantic', np.float64(0.4217647821447532)), ('terms', np.float64(0.4217647821447532)), ('capture', np.float64(0.4217647821447532)), ('embeddings', np.float64(0.4217647821447532))]\n","Result: Transformers use attention mechanisms to model relationships between words.Score: 0.267\n","Result: Large language models are revolutionizing natural language processing.Score: 0.236\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# A small corpus of documents\n","documents = [\n","    'Large language models are revolutionizing natural language processing.',\n","    'Transformers use attention mechanisms to model relationships between words.',\n","    'TF–IDF vectors provide a baseline for information retrieval.',\n","    'Word embeddings capture semantic information about terms.',\n","]\n","\n","# Build the vectoriser and transform the corpus\n","vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = vectorizer.fit_transform(documents)\n","\n","# Function to search the corpus given a query\n","def search(query: str, top_k: int = 2):\n","    \"\"\"Compute similarity between the query and each document and return the top_k documents.\"\"\"\n","    query_vec = vectorizer.transform([query])\n","    sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n","    top_indices = sims.argsort()[::-1][:top_k]\n","    return [(documents[i], sims[i]) for i in top_indices]\n","\n","# Inspect the most significant terms in each document\n","feature_names = vectorizer.get_feature_names_out()\n","for idx, doc in enumerate(documents):\n","    vector = tfidf_matrix[idx].toarray().flatten()\n","    top_terms_idx = vector.argsort()[::-1][:5]\n","    top_terms = [(feature_names[i], vector[i]) for i in top_terms_idx]\n","    print(f'Document {idx} top terms:', top_terms)\n","\n","# Example search\n","results = search('attention models')\n","for doc, score in results:\n","    print(f'Result: {doc}Score: {score:.3f}')\n"]},{"cell_type":"markdown","id":"168345a3","metadata":{"id":"168345a3"},"source":["### Discussion\n","\n","In the example above we vectorised a handful of short documents using TF‑IDF and explored the top weighted terms for each. The function `search` computes cosine similarity between a query and all documents and returns the top hits. Note how terms with low overall frequency (e.g. \"transformers\") tend to have high TF‑IDF weights, making them influential in similarity calculations.\n","\n","However, TF‑IDF does not capture deeper semantic relationships or synonyms. For instance, a query containing \"neural network\" would not match a document about \"deep learning\" because the words differ. To address this, dense vector representations trained with neural networks can embed related words close together in a high‑dimensional space. The next section introduces this idea. [9]"]},{"cell_type":"markdown","id":"27719b7e","metadata":{"id":"27719b7e"},"source":["## Part 2 – Dense Embeddings (Reference)\n","\n","Dense embeddings map sentences or documents into a continuous vector space such that semantically similar texts have vectors that are close together. Modern models like those provided by the `sentence-transformers` library (based on transformer architectures) produce embeddings that capture meaning beyond individual word counts. These embeddings can be indexed with libraries such as Facebook AI Similarity Search (FAISS) to enable efficient nearest‑neighbour search across millions of vectors.\n","\n","To use dense embeddings you typically need to:\n","1. **Install dependencies:** `pip install sentence-transformers faiss-cpu` (or `faiss-gpu` if you have a GPU).\n","2. **Load a pre‑trained model:** e.g. `'all-MiniLM-L6-v2'` is a compact model for sentence embeddings.\n","3. **Encode your documents:** Pass each sentence or paragraph through the model to obtain a 384‑dimensional vector.\n","4. **Index the vectors:** Build a FAISS index to support fast similarity search.\n","5. **Query the index:** Encode the query and retrieve the most similar documents along with their scores.\n","\n","Below is an example illustrating these steps (the code is presented as reference and will not run in this environment without the required libraries). [9]"]},{"cell_type":"code","execution_count":4,"id":"44b3090e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8902,"status":"ok","timestamp":1760116111570,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"},"user_tz":420},"id":"44b3090e","outputId":"eccaa9b6-72d4-4998-87a0-f8c5e57ad26e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n","Result: Transformers use attention mechanisms to model relationships between words. (distance=0.4766)\n","Result: Large language models are revolutionizing natural language processing. (distance=1.4741)\n"]}],"source":["!pip install sentence-transformers faiss-cpu\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import numpy as np\n","\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# # Encode the corpus into dense vectors\n","dense_vectors = model.encode(documents, convert_to_tensor=False, show_progress_bar=False)\n","dense_vectors = np.array(dense_vectors).astype('float32')\n","\n","# Build a FAISS index\n","dim = dense_vectors.shape[1]\n","index = faiss.IndexFlatL2(dim)\n","index.add(dense_vectors)\n","\n","# Encode the query and perform search\n","query_embedding = model.encode(['attention in transformers'], convert_to_tensor=False)\n","query_embedding = np.array(query_embedding).astype('float32')\n","distances, indices = index.search(query_embedding, k=2)\n","for dist, idx in zip(distances[0], indices[0]):\n","    print(f'Result: {documents[idx]} (distance={dist:.4f})')\n"]},{"cell_type":"markdown","id":"0f301b4c","metadata":{"id":"0f301b4c"},"source":["## Exercises\n","\n","1. **Explore different corpora:** Replace the `documents` list with sentences from a news article, a technical report or your own writing. Observe how TF‑IDF weights and top terms change.\n","2. **Implement a document ranking:** Modify the `search` function to return the top 3 documents instead of 2. For each query, explain why certain documents rank higher or lower.\n","3. **Compare TF‑IDF with dense embeddings:** Install `sentence-transformers` and `faiss-cpu` in your own environment. Encode the same documents with a sentence embedding model and perform a similarity search. Compare the results with the TF‑IDF system. Which queries benefit most from dense embeddings?\n","4. **Synonym handling:** Create queries that use synonyms (e.g. 'artificial intelligence' vs 'AI'). Evaluate how TF‑IDF and dense embeddings handle these synonyms differently.\n","5. **Extend to multi‑paragraph documents:** Group multiple sentences into longer documents (e.g. full paragraphs). Recompute TF‑IDF vectors and observe how term weights distribute across larger contexts. What challenges arise when documents vary greatly in length?\n"]},{"cell_type":"markdown","source":["Foundational LLMs & Transformers\n","1. Vaswani, A., et al. (2017). Attention is All You Need. Advances in Neural Information Processing Systems (NIPS 2017).\n","2. Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS 2020.\n","3. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT 2019.\n","4. OpenAI (2023). GPT-4 Technical Report. arXiv:2303.08774.\n","5. Touvron, H., et al. (2023). LLaMA 2: Open Foundation and Fine-Tuned Chat Models. Meta AI.\n","\n","Generative AI & Sampling\n","\n","6. Goodfellow, I., et al. (2014). Generative Adversarial Nets. NeurIPS 2014.\n","7. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\n","8. Neal, R. M. (1993). Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical Report CRG-TR-93-1, University of Toronto.\n","\n","Retrieval-Augmented Generation (RAG) & Knowledge Grounding\n","\n","9. Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP. NeurIPS 2020.\n","10. deepset ai (2023). Haystack: Open-Source Framework for Search and RAG Applications. https://haystack.deepset.ai\n","11. LangChain (2023). LangChain Documentation and Cookbook. https://python.langchain.com\n","\n","Evaluation & Safety\n","\n","12. Papineni, K., et al. (2002). BLEU: A Method for Automatic Evaluation of Machine Translation. ACL 2002.\n","13. Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. ACL Workshop 2004.\n","14. OpenAI (2024). Evaluating Model Outputs: Faithfulness and Grounding. OpenAI Docs.\n","15. Guardrails AI (2024). Open-Source Guardrails Framework. https://github.com/shreyar/guardrails\n","\n","Prompt Engineering & Instruction Tuning\n","\n","16. White, J. (2023). The Prompting Guide. https://www.promptingguide.ai\n","17. Ouyang, L., et al. (2022). Training Language Models to Follow Instructions with Human Feedback. NeurIPS 2022.\n","\n","Agents & Tool Use\n","\n","18. Yao, S., et al. (2022). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629.\n","19. LangChain (2024). LangChain Agents and Tools Documentation.\n","20. Microsoft (2023). Semantic Kernel Developer Guide. https://learn.microsoft.com/en-us/semantic-kernel/\n","21. Google DeepMind (2024). Gemini Technical Report. arXiv:2312.11805.\n","\n","State, Memory & Orchestration\n","\n","22. LangGraph (2024). Stateful Agent Orchestration Framework. https://langchain-langgraph.vercel.app\n","23. Park, J. S., et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442.\n","\n","Pedagogical and Course Design References\n","\n","24. fast.ai (2023). fast.ai Deep Learning Course Notebooks. https://course.fast.ai\n","25. Ng, A. (2023). DeepLearning.AI Short Courses on Generative AI.\n","26. MIT 6.S191, Stanford CS324, UC Berkeley CS294-158. (2022–2024). Course Materials and Public Notebooks for ML and LLMs."],"metadata":{"id":"kIEtQvFdg4lI"},"id":"kIEtQvFdg4lI"},{"cell_type":"code","source":[],"metadata":{"id":"gGVSlZjzg9wd","executionInfo":{"status":"ok","timestamp":1760116111576,"user_tz":420,"elapsed":2,"user":{"displayName":"Aryan Mistry","userId":"02642917390118854248"}}},"id":"gGVSlZjzg9wd","execution_count":4,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}